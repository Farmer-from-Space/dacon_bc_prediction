{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cb6766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
      "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /private/var/folders/5m/j1nv4tbn34l6v5n5hjqzx0c40000gn/T/pip-req-build-hcdoyg27\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /private/var/folders/5m/j1nv4tbn34l6v5n5hjqzx0c40000gn/T/pip-req-build-hcdoyg27\n",
      "  Resolved https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to commit 6b5e8953a80aef5b324104dc0c2e9b8c34d622bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47032c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet-pytorch in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (from efficientnet-pytorch) (1.14.0.dev20221108)\n",
      "Requirement already satisfied: networkx in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (3.0b1)\n",
      "Requirement already satisfied: sympy in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (from torch->efficientnet-pytorch) (4.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/krc/miniforge3/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd1170e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from efficientnet_pytorch import model as enet\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706a277",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## resize (background delete) - K means - score evaluation - apply crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d9caf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/krc/Documents/breast_dacon/dacon_bc_prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67aa31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import copy \n",
    "\n",
    "#From t.ly/TLq_\n",
    "\n",
    "\n",
    "class KMeans():\n",
    "    def __init__(self, n_clusters, max_iter = 20, random_state = 60):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def initialize_centroids(self, X): # center 초기화 - 이미지 내 특정 점으로 고정\n",
    "        np.random.RandomState(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "\n",
    "        return centroids\n",
    "        \n",
    "    def compute_centroids(self, X, labels): # centroids 계산 \n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "        \n",
    "    def compute_distance(self, X, centroids): #\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "    \n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "    \n",
    "    def compute_sse(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    " \n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids\n",
    "            distance = self.compute_distance(X, old_centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "            self.centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "        self.error = self.compute_sse(X, self.labels, self.centroids)\n",
    "        # return self.labels\n",
    "        \n",
    "    def predict(self, X):\n",
    "        distance = self.compute_distance(X, self.centroids)\n",
    "        return self.find_closest_cluster(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2546d677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run \n",
    "\n",
    "# file_path = '/Users/krc/Documents/breast_dacon/train_imgs/'\n",
    "# file_list = os.walk(file_path)  # OS로 불러오기\n",
    "# img_files = [file for file in file_list if file[-1][-1].endswith(\".png\")]\n",
    "\n",
    "\n",
    "\n",
    "# if not img_files:  # if empty folder\n",
    "#     print(\"there are no png files\")\n",
    "#     sys.exit()\n",
    "\n",
    "# for i, f in enumerate(img_files[0][2]):\n",
    "#     if i% 20 == 0:\n",
    "#         print(f'index {i} is starting.. ')\n",
    "#     # t.ly/zgLP\n",
    "#     image = cv2.imread(file_path + f)\n",
    "#     y_orig, x_orig, channel= image.shape\n",
    "#     if x_orig / y_orig < 1.5:\n",
    "#         cv2.imwrite(f'./new_train/{f}', image)\n",
    "#         print(f'{f} has one image')\n",
    "#         continue\n",
    "#     gray_sample = image.copy()\n",
    "#     gray_sample = cv2.cvtColor(gray_sample, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "#     # 모양 맞추기 flip \n",
    "#     gray_sample =cv2.flip(gray_sample, 0)\n",
    "#     # resize to 400 * 200\n",
    "#     gray_sample = cv2.resize(gray_sample, dsize=(400, 200), interpolation=cv2.INTER_LINEAR) \n",
    "    \n",
    "#     #전처리\n",
    "#     coord = np.where( gray_sample < 239 )\n",
    "#     co_array = np.array(coord)\n",
    "#     co_array = np.float32(co_array).T\n",
    "#     from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "#     best_score = 0\n",
    "#     best_k = 1\n",
    "#     best_centroids = []\n",
    "\n",
    "#     for j, k in enumerate([ 4, 3, 2]):\n",
    "        \n",
    "#         # Run the Kmeans algorithm\n",
    "#         km = KMeans(n_clusters=k)\n",
    "#         km.fit(co_array)\n",
    "#         labels = km.predict(co_array) # input data\n",
    "\n",
    "#         centroids = km.compute_centroids(co_array, labels) # cluster_centers_\n",
    "#         # Get silhouette samples\n",
    "#         silhouette_vals = silhouette_samples(co_array, labels)\n",
    "\n",
    "#         # Get the average silhouette score and plot it\n",
    "#         avg_score = np.mean(silhouette_vals) ## score \n",
    "\n",
    "#         # print('K = ',k, 'avg_score:', avg_score)\n",
    "\n",
    "#         if best_score < avg_score:\n",
    "#             if best_k == 3 and k ==2 and (avg_score-best_score) < 0.09:\n",
    "#                 break\n",
    "#             best_score = avg_score\n",
    "#             best_k = k\n",
    "#             best_centroids = centroids\n",
    "#             x_coord = centroids[ : , 1]\n",
    "#             x_coord.sort()        \n",
    "#             if best_k == 2:\n",
    "#                 x1, x2 = x_coord\n",
    "#             elif best_k ==3  :\n",
    "#                 x1, x2, x3 = x_coord\n",
    "#             elif best_k ==4  :\n",
    "#                 x1, x2, x3, x4 = x_coord\n",
    "\n",
    "#     if best_k == 2:\n",
    "#         crop_image = image[:, : int( (x1+x2)/2  *x_orig / 400 ), :]\n",
    "#     elif best_k > 2:\n",
    "#         alpha = int(y_orig / 2)\n",
    "#         crop_image = image[:, max(0, int(x2 *x_orig /400) -alpha)  :int(x2 *x_orig /400) +alpha , : ] ## 마이너스나옴 \n",
    "\n",
    "#     cv2.imwrite(f'./new_train/{f}', crop_image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26304322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test imgs run \n",
    "# file_path = '/Users/krc/Documents/breast_dacon/test_imgs/'\n",
    "# file_list = os.walk(file_path)  # OS로 불러오기\n",
    "# img_files = [file for file in file_list if file[-1][-1].endswith(\".png\")]\n",
    "\n",
    "\n",
    "\n",
    "# if not img_files:  # if empty folder\n",
    "#     print(\"there are no png files\")\n",
    "#     sys.exit()\n",
    "\n",
    "# for i, f in enumerate(img_files[0][2]):\n",
    "#     if i% 20 == 0:\n",
    "#         print(f'index {i} is starting.. ')\n",
    "#     # t.ly/zgLP\n",
    "#     image = cv2.imread(file_path + f)\n",
    "#     y_orig, x_orig, channel= image.shape\n",
    "#     if x_orig / y_orig < 1.5:\n",
    "#         cv2.imwrite(f'./new_test/{f}', image)\n",
    "#         print(f'image {f} has one image')\n",
    "#         continue\n",
    "#     gray_sample = image.copy()\n",
    "#     gray_sample = cv2.cvtColor(gray_sample, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "#     # 모양 맞추기 flip \n",
    "#     gray_sample =cv2.flip(gray_sample, 0)\n",
    "#     # resize to 400 * 200\n",
    "#     gray_sample = cv2.resize(gray_sample, dsize=(400, 200), interpolation=cv2.INTER_LINEAR) \n",
    "    \n",
    "#     #전처리\n",
    "#     coord = np.where( gray_sample < 239 )\n",
    "#     co_array = np.array(coord)\n",
    "#     co_array = np.float32(co_array).T\n",
    "#     from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "#     best_score = 0\n",
    "#     best_k = 1\n",
    "#     best_centroids = []\n",
    "\n",
    "#     for j, k in enumerate([ 4, 3, 2]):\n",
    "        \n",
    "#         # Run the Kmeans algorithm\n",
    "#         km = KMeans(n_clusters=k)\n",
    "#         km.fit(co_array)\n",
    "#         labels = km.predict(co_array) # input data\n",
    "\n",
    "#         centroids = km.compute_centroids(co_array, labels) # cluster_centers_\n",
    "#         # Get silhouette samples\n",
    "#         silhouette_vals = silhouette_samples(co_array, labels)\n",
    "\n",
    "#         # Get the average silhouette score and plot it\n",
    "#         avg_score = np.mean(silhouette_vals) ## score \n",
    "\n",
    "#         # print('K = ',k, 'avg_score:', avg_score)\n",
    "\n",
    "#         if best_score < avg_score:\n",
    "#             if best_k == 3 and k ==2 and (avg_score-best_score) < 0.09:\n",
    "#                 break\n",
    "#             best_score = avg_score\n",
    "#             best_k = k\n",
    "#             best_centroids = centroids\n",
    "#             x_coord = centroids[ : , 1]\n",
    "#             x_coord.sort()        \n",
    "#             if best_k == 2:\n",
    "#                 x1, x2 = x_coord\n",
    "#             elif best_k ==3  :\n",
    "#                 x1, x2, x3 = x_coord\n",
    "#             elif best_k ==4  :\n",
    "#                 x1, x2, x3, x4 = x_coord\n",
    "\n",
    "#     if best_k == 2:\n",
    "#         crop_image = image[:, : int( (x1+x2)/2  *x_orig / 400 ), :]\n",
    "#     elif best_k > 2:\n",
    "#         alpha = int(y_orig / 2)\n",
    "#         crop_image = image[:, max(0, int(x2 *x_orig /400) -alpha)  :int(x2 *x_orig /400) +alpha , : ] ## 마이너스나옴 \n",
    "\n",
    "#     cv2.imwrite(f'./new_test/{f}', crop_image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216f0ab",
   "metadata": {},
   "source": [
    "--- \n",
    "## Model input\n",
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7272da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./new_train\n"
     ]
    }
   ],
   "source": [
    "data_dir = './new_train'\n",
    "df_train = pd.read_csv('/Users/krc/Documents/breast_dacon/open/train.csv')\n",
    "df_test = pd.read_csv('/Users/krc/Documents/breast_dacon/open/test.csv')\n",
    "\n",
    "image_folder = data_dir\n",
    "\n",
    "enet_type = 'efficientnet-b4'\n",
    "fold=0\n",
    "tile_size = 256\n",
    "image_size = 256\n",
    "n_tiles = 36\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "out_dim = 5\n",
    "init_lr  = 3e-4\n",
    "warmup_factor = 10\n",
    "\n",
    "warmup_epo = 1\n",
    "n_epochs = 20\n",
    "\n",
    "print(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5533805",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6b68e",
   "metadata": {},
   "source": [
    "## Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c50bdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_path</th>\n",
       "      <th>나이</th>\n",
       "      <th>수술연월일</th>\n",
       "      <th>진단명</th>\n",
       "      <th>암의 위치</th>\n",
       "      <th>암의 개수</th>\n",
       "      <th>암의 장경</th>\n",
       "      <th>NG</th>\n",
       "      <th>...</th>\n",
       "      <th>PR</th>\n",
       "      <th>PR_Allred_score</th>\n",
       "      <th>KI-67_LI_percent</th>\n",
       "      <th>HER2</th>\n",
       "      <th>HER2_IHC</th>\n",
       "      <th>HER2_SISH</th>\n",
       "      <th>HER2_SISH_ratio</th>\n",
       "      <th>BRCA_mutation</th>\n",
       "      <th>N_category</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BC_01_0001</td>\n",
       "      <td>./train_imgs/BC_01_0001.png</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>2015-10-23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BC_01_0002</td>\n",
       "      <td>./train_imgs/BC_01_0002.png</td>\n",
       "      <td>-</td>\n",
       "      <td>51</td>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC_01_0003</td>\n",
       "      <td>./train_imgs/BC_01_0003.png</td>\n",
       "      <td>-</td>\n",
       "      <td>37</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BC_01_0004</td>\n",
       "      <td>./train_imgs/BC_01_0004.png</td>\n",
       "      <td>-</td>\n",
       "      <td>54</td>\n",
       "      <td>2016-03-08</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BC_01_0005</td>\n",
       "      <td>./train_imgs/BC_01_0005.png</td>\n",
       "      <td>-</td>\n",
       "      <td>57</td>\n",
       "      <td>2015-10-30</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                     img_path mask_path  나이       수술연월일  진단명  \\\n",
       "0  BC_01_0001  ./train_imgs/BC_01_0001.png         -  63  2015-10-23    1   \n",
       "1  BC_01_0002  ./train_imgs/BC_01_0002.png         -  51  2015-10-28    1   \n",
       "2  BC_01_0003  ./train_imgs/BC_01_0003.png         -  37  2015-10-29    1   \n",
       "3  BC_01_0004  ./train_imgs/BC_01_0004.png         -  54  2016-03-08    1   \n",
       "4  BC_01_0005  ./train_imgs/BC_01_0005.png         -  57  2015-10-30    1   \n",
       "\n",
       "   암의 위치  암의 개수  암의 장경   NG  ...   PR  PR_Allred_score  KI-67_LI_percent  \\\n",
       "0      2      1   19.0  2.0  ...  1.0              6.0              12.0   \n",
       "1      1      1   22.0  3.0  ...  0.0              NaN              70.0   \n",
       "2      2      1    NaN  2.0  ...  1.0              4.0               7.0   \n",
       "3      2      1    0.0  3.0  ...  0.0              NaN               1.0   \n",
       "4      2      1    8.0  2.0  ...  0.0              NaN               8.0   \n",
       "\n",
       "   HER2  HER2_IHC  HER2_SISH  HER2_SISH_ratio  BRCA_mutation  N_category  fold  \n",
       "0   0.0       1.0        NaN              NaN            NaN           0     4  \n",
       "1   0.0       0.0        NaN              NaN            NaN           1     2  \n",
       "2   0.0       1.0        NaN              NaN            0.0           0     2  \n",
       "3   1.0       3.0        NaN              NaN            NaN           0     4  \n",
       "4   1.0       2.0        1.0             5.44            NaN           0     4  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(5, shuffle = True, random_state=42)\n",
    "df_train['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train['N_category'])):\n",
    "    df_train.loc[valid_idx, 'fold'] = i\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae259f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class enetv4(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(evetv4, self).__init__()\n",
    "        self.enet = models.efficientnet_b4(weights=True)\n",
    "        \n",
    "        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n",
    "        self.enet._fc = nn.Identity()\n",
    "        \n",
    "    def extract(self, x):\n",
    "        return self.enet(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= self.extract(x)\n",
    "        x = self.myfc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68455200",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "754b084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(img, mode=0):\n",
    "    result = []\n",
    "    h, w, c = img.shape\n",
    "    pad_h = (tile_size -h % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "    pad_w = (tile_size -w % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "    \n",
    "    img2 = np.pad(img, [[pad_h // 2, pad_h - pad_h//2], [pad_w //2, pad_w - pad_w //2], [0,0]], constant_values = 255)\n",
    "    img3 = img2.reshape( \n",
    "                        img2.shape[0] // tile_size, \n",
    "                        tile_size, \n",
    "                        img2.shape[1] // tile_size, \n",
    "                        tile_size,\n",
    "                        3 )\n",
    "    img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size, 3)\n",
    "    n_tiles_with_info = (img3.reshape(img3.shape[0], -1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n",
    "    if len(img3) < n_tiles :\n",
    "        img3 = np.pad(img3, [[0, n_tiles -len(img3)], [0,0], [0,0], [0,0]], constant_values = 255)\n",
    "    idxs = np.argsort(img3.reshape(img3.shape[0], -1).sum(-1))[:n_tiles]\n",
    "    img3 = img3[idxs]\n",
    "    for i in range(len(img3)):\n",
    "        result.append({'img':img3[i], 'idx':i})\n",
    "    return result, n_tiles_with_info >= n_tiles\n",
    "        \n",
    "        \n",
    "class PANDADataset(Dataset):\n",
    "        def __init__(self, df, image_size, n_tiles = n_tiles, tile_mode =0, rand=False, transform = None):\n",
    "        \n",
    "            self.df = df.reset_index(drop = True)\n",
    "            self.image_size = image_size\n",
    "            self.n_tiles = n_tiles\n",
    "            self.tile_mode = tile_mode\n",
    "            self.rand = rand\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.df.shape[0]\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            row = self.df.iloc[index]\n",
    "            #img_id = row.image_id\n",
    "        \n",
    "\n",
    "            tiff_file = image_folder + self.df['img_path'].iloc[index][-15:]\n",
    "            image = skimage.io.imread(tiff_file) #image = skimage.io.MultiImage(tiff_file)[1]\n",
    "            # resize \n",
    "            shape_w, shapw_h, _ = image.shape\n",
    "            image = resize(image, (min(shape_w, shapw_h),min(shape_w, shapw_h)), anti_aliasing = True) \n",
    "            \n",
    "            tiles, OK = get_tiles(image, self.tile_mode)\n",
    "        \n",
    "            if self.rand:\n",
    "                idxes = np.random.choide(lit(range(self.n_tiles)), self.n_tiles, replace = False)\n",
    "            else:\n",
    "                idxes = list(range(self.n_tiles))\n",
    "                \n",
    "            n_row_tiles = int(np.sqrt(self.n_tiles))\n",
    "            images = np.zeros((image_size * n_row_tiles, image_size * n_row_tiles, 3))\n",
    "            \n",
    "            for h in range(n_row_tiles):\n",
    "                for w in range(n_row_tiles):\n",
    "                    i = h* n_row_tiles + w\n",
    "                    \n",
    "                    if len(tiles) > idxes[i]:\n",
    "                        this_img = tiles[idxes[i]]['img']\n",
    "                    else:\n",
    "                        this_img = np.ones((self.image_size, self_image_size, 3)).astype(np.uint8) * 255\n",
    "                    \n",
    "                    this_img = 255 - this_img\n",
    "                    if self.transform is not None:\n",
    "                        this_img = self.transform(image = this_img)['image']\n",
    "                    h1 = h * image_size\n",
    "                    w1 = w * image_size\n",
    "                    images[h1:h1+image_size, w1:w1+image_size] = this_img \n",
    "                    \n",
    "            if self.transform is not None:\n",
    "                images = self.transform(image = images)['image']\n",
    "            images = images.astype(np.float32)\n",
    "            images /= 255\n",
    "            images = images.transpose(2,0,1)\n",
    "            \n",
    "            label = np.zeros(1).astype(np.float32) ## 정답 데이터 class 개수  \n",
    "            label[:row.N_category]  = 1.\n",
    "            return torch.tensor(images), torch.tensor(label)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9effde5b",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9f0929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.Transpose( p=0.5),\n",
    "    albumentations.VerticalFlip( p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "])\n",
    "transforms_val = albumentations.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c72fc39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'isup_grade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_show \u001b[38;5;241m=\u001b[39m PANDADataset(df_train, image_size, n_tiles, \u001b[38;5;241m0\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransforms_train)\n\u001b[0;32m----> 3\u001b[0m i, j\u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataset_show)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(i, j))\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mPANDADataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     79\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 82\u001b[0m label[:\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misup_grade\u001b[49m]  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(images), torch\u001b[38;5;241m.\u001b[39mtensor(label)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'isup_grade'"
     ]
    }
   ],
   "source": [
    "dataset_show = PANDADataset(df_train, image_size, n_tiles, 0, transform=transforms_train)\n",
    "\n",
    "i, j= iter(dataset_show)\n",
    "print(next(i, j))\n",
    "# from pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 20,10\n",
    "# for i in range(2):\n",
    "#     f, axarr = plt.subplots(1,5)\n",
    "#     for p in range(5):\n",
    "#         idx = np.random.randint(0, len(dataset_show))\n",
    "#         img, label = dataset_show[idx]\n",
    "#         axarr[p].imshow(1. - img.transpose(0,1).transpose(1,2).squeeze())\n",
    "#         axarr[p].set_title(str(sum(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba6973f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_array = np.array(np.random.randint(10, size = (10,10)))\n",
    "temp_array.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd018c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efff6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "    torch.backends.mps.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bc0a1",
   "metadata": {},
   "source": [
    "### data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6cf44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv('../open/train.csv')\n",
    "test_df = pd.read_csv('../open/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16cb23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, train_labels, val_labels = train_test_split(\n",
    "                                                    train_df.drop(columns=['N_category']), \n",
    "                                                    train_df['N_category'], \n",
    "                                                    test_size=0.1,  # 0.2 \n",
    "                                                    random_state=CFG['SEED']\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0deb101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './new_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78daf768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./new_train/BC_01_2794.png'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base + train_df['img_path'].iloc[0][12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac860ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, medical_df, labels, transforms=None):\n",
    "        self.medical_df = medical_df\n",
    "        self.transforms = transforms\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = base + self.medical_df['img_path'].iloc[index][12:]\n",
    "        # print(img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "                \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return image,  label #tabular = 2nd\n",
    "        else:\n",
    "            return image #, tabular\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.medical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81e4d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "                            A.HorizontalFlip(),\n",
    "                            A.VerticalFlip(),\n",
    "                            A.Rotate(limit=180, border_mode=cv2.BORDER_CONSTANT,p=0.3),\n",
    "                            A.GaussNoise(p=0.2), #NOISE ADD \n",
    "                            A.Resize(CFG['IMG_SIZE_D'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE_D'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35114ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, train_labels.values, train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_df, val_labels.values, test_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474c887",
   "metadata": {},
   "source": [
    "## model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "047fcd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImgFeatureExtractor, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=True)\n",
    "        for i, param in enumerate(self.backbone.parameters()):\n",
    "              if i >139:\n",
    "                param.requires_grad= True\n",
    "        self.embedding = nn.Linear(1000,512)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f48c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.img_feature_extractor = ImgFeatureExtractor()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, img): \n",
    "        img_feature = self.img_feature_extractor(img)\n",
    "        output = self.classifier(img_feature) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66c50857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_epcoh = 0 \n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for img, label in tqdm(iter(train_loader)): \n",
    "            img = img.float().to(device)\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model_pred = model(img)\n",
    "            \n",
    "            loss = criterion(model_pred, label.reshape(-1,1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, val_score = validation(model, criterion, val_loader, device)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_score)\n",
    "        \n",
    "        if best_score < val_score:\n",
    "            best_epoch = epoch\n",
    "            best_score = val_score\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), f'./{epoch}_model.pth')\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "579188d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "    val_loss = []\n",
    "    threshold = 0.5\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(val_loader)): \n",
    "            true_labels += label.tolist()\n",
    "            \n",
    "            img = img.float().to(device)\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            model_pred = model(img)\n",
    "            \n",
    "            loss = criterion(model_pred, label.reshape(-1,1))\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            model_pred = model_pred.squeeze(1).to('cpu')  \n",
    "            pred_labels += model_pred.tolist()\n",
    "    \n",
    "    pred_labels = np.where(np.array(pred_labels) > threshold, 1, 0)\n",
    "    val_score = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n",
    "    return np.mean(val_loss), val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aef2a4a663e4ee0bb40bca6819c9b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn.DataParallel(ClassificationModel())\n",
    "\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa76c5",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_df, None, test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b48e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    threshold = 0.5\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)): # , tabular   = 2nd\n",
    "            img = img.float().to(device)\n",
    "            # tabular = tabular.float().to(device)\n",
    "            \n",
    "            model_pred = model(img)\n",
    "            \n",
    "            model_pred = model_pred.squeeze(1).to('cpu')\n",
    "            \n",
    "            preds += model_pred.tolist()\n",
    "    \n",
    "    preds = np.where(np.array(preds) > threshold, 1, 0)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ebb2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f576d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['N_category'] = preds\n",
    "submit.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dce53c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(np.zeros(50))\n",
    "b = list(np.ones(50))\n",
    "answer = a+b\n",
    "\n",
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0974a4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = list(np.random.randint(2, size=(100,1)))\n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a9a6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5148514851485149"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.f1_score(answer, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35b7f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(5)\n",
    "label[:] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e112e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aeac38f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[5].N_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284feb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
