{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.listdir(\"../data/train\")\n",
    "test = os.listdir(\"../data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Tumer\", \"Invasive Cancer\", \"else\"]\n",
    "\n",
    "class_values = [[0, 0, 0], [75, 75,75], [255, 255, 255]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tile(im_list_2d):\n",
    "    return cv2.vconcat([cv2.hconcat(im_list_h) for im_list_h in im_list_2d])\n",
    "\n",
    "\n",
    "def cut_image(img, scale):\n",
    "    # make image bag\n",
    "    # 이미지가 너무 클 경우 gpu resource가 부족한 문제가 생김\n",
    "    # overlap tile stragey를 사용\n",
    "    img_bag = []\n",
    "    size = img.shape\n",
    "    h_num = size[1] // scale - 1\n",
    "    w_num = size[2] // scale - 1\n",
    "    for i in range(h_num):\n",
    "        tmp = []\n",
    "        for j in range(w_num):\n",
    "            cropped_img = img[:, i*scale:(i+1)*scale+256, j*scale:(j+1)*scale+256]\n",
    "            tmp.append(cropped_img)\n",
    "        img_bag.append(tmp)\n",
    "\n",
    "\n",
    "    return img_bag, h_num, w_num\n",
    "\n",
    "\n",
    "def crop_image(image, target_image_dims=[256,256,3]):\n",
    "   \n",
    "    target_size = target_image_dims[0]\n",
    "    image_size = len(image)\n",
    "    padding = (image_size - target_size) // 2\n",
    "\n",
    "    return image[\n",
    "        padding:image_size - padding,\n",
    "        padding:image_size - padding,\n",
    "        :,\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            base_path,\n",
    "            class_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.image_paths = [os.path.join(base_path, image_id) for image_id in images_dir]\n",
    "        self.image_id = images_dir\n",
    "        self.class_values = class_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        size= image.shape\n",
    "        h = 256 - size[0] % 256\n",
    "        w = 256 - size[1] % 256\n",
    "        image = np.pad(image, pad_width=[(128, h+128),(128, w+128),(0, 0)], mode=\"reflect\")\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image = sample['image']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            \n",
    "        return image, size, self.image_id[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute ratio with model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = class_names\n",
    "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('./unet_best_model1.pth'):\n",
    "    best_model = torch.load('./unet_best_model1.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from a previous commit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = BuildingsDataset(\n",
    "    test, \n",
    "    \"../data/test\", \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_values=class_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
    "train_dataset = BuildingsDataset(\n",
    "    train, \n",
    "    \"../data/train\", \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_values=class_values,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio1 = []\n",
    "\n",
    "for idx in range(len(train_dataset)):\n",
    "    print(idx)\n",
    "    image, size, path = train_dataset[idx]\n",
    "    image_bag, h_num, w_num = cut_image(image, 256)\n",
    "    mask_bag = []\n",
    "    for i in range(h_num):\n",
    "        tmp = []\n",
    "        for j in range(w_num):\n",
    "            img = image_bag[i][j]\n",
    "            x_tensor = torch.from_numpy(img).to(DEVICE).unsqueeze(0)\n",
    "            # Predict test image\n",
    "            pred_mask = best_model(x_tensor)\n",
    "            pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "            pred_mask = pred_mask[:, 128:-128, 128:-128]\n",
    "            # Convert pred_mask from `CHW` format to `HWC` format\n",
    "            pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "            # Get prediction channel corresponding to building\n",
    "            pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), class_values), target_image_dims = [size[1], size[2], size[0]])\n",
    "            tmp.append(pred_mask)\n",
    "        mask_bag.append(tmp)\n",
    "\n",
    "    mask = concat_tile(mask_bag)\n",
    "    mask = mask[:size[0], :size[1], :]\n",
    "    \n",
    "    cancer = np.sum(mask == 75)\n",
    "    tumor = np.sum(mask == 0)\n",
    "    train_ratio1.append([path, cancer/(cancer+tumor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio1 = pd.DataFrame(train_ratio1, columns=[\"path\", \"ratio\"])\n",
    "train_ratio1.to_csv(\"./train_ratio1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio1 = []\n",
    "for idx in range(len(test_dataset)):\n",
    "    print(idx)\n",
    "    image, size, path = test_dataset[idx]\n",
    "    image_bag, h_num, w_num = cut_image(image, 256)\n",
    "    mask_bag = []\n",
    "    for i in range(h_num):\n",
    "        tmp = []\n",
    "        for j in range(w_num):\n",
    "            img = image_bag[i][j]\n",
    "            x_tensor = torch.from_numpy(img).to(DEVICE).unsqueeze(0)\n",
    "            # Predict test image\n",
    "            pred_mask = best_model(x_tensor)\n",
    "            pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "            pred_mask = pred_mask[:, 128:-128, 128:-128]\n",
    "            # Convert pred_mask from `CHW` format to `HWC` format\n",
    "            pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "            # Get prediction channel corresponding to building\n",
    "            pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), class_values), target_image_dims = [size[1], size[2], size[0]])\n",
    "            tmp.append(pred_mask)\n",
    "        mask_bag.append(tmp)\n",
    "\n",
    "    mask = concat_tile(mask_bag)\n",
    "    mask = mask[:size[0], :size[1], :]\n",
    "\n",
    "    cancer = np.sum(mask == 75)\n",
    "    tumor = np.sum(mask == 0)\n",
    "    test_ratio1.append([path, cancer/(cancer+tumor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio1 = pd.DataFrame(test_ratio1, columns=[\"path\", \"ratio\"])\n",
    "test_ratio1.to_csv(\"./test_ratio1.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute ratio with model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = class_names\n",
    "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('./unet_best_model2.pth'):\n",
    "    best_model = torch.load('./unet_best_model2.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from a previous commit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = BuildingsDataset(\n",
    "    test, \n",
    "    \"../data/test\", \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_values=class_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
    "train_dataset = BuildingsDataset(\n",
    "    train, \n",
    "    \"../data/train\", \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_values=class_values,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio2 = []\n",
    "\n",
    "for idx in range(len(train_dataset)):\n",
    "    print(idx)\n",
    "    image, size, path = train_dataset[idx]\n",
    "    image_bag, h_num, w_num = cut_image(image, 256)\n",
    "    mask_bag = []\n",
    "    for i in range(h_num):\n",
    "        tmp = []\n",
    "        for j in range(w_num):\n",
    "            img = image_bag[i][j]\n",
    "            x_tensor = torch.from_numpy(img).to(DEVICE).unsqueeze(0)\n",
    "            # Predict test image\n",
    "            pred_mask = best_model(x_tensor)\n",
    "            pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "            pred_mask = pred_mask[:, 128:-128, 128:-128]\n",
    "            # Convert pred_mask from `CHW` format to `HWC` format\n",
    "            pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "            # Get prediction channel corresponding to building\n",
    "            pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), class_values), target_image_dims = [size[1], size[2], size[0]])\n",
    "            tmp.append(pred_mask)\n",
    "        mask_bag.append(tmp)\n",
    "\n",
    "    mask = concat_tile(mask_bag)\n",
    "    mask = mask[:size[0], :size[1], :]\n",
    "    \n",
    "    cancer = np.sum(mask == 75)\n",
    "    tumor = np.sum(mask == 0)\n",
    "    train_ratio2.append([path, cancer/(cancer+tumor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio2 = pd.DataFrame(train_ratio2, columns=[\"path\", \"ratio\"])\n",
    "train_ratio2.to_csv(\"./train_ratio2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio2 = []\n",
    "for idx in range(len(test_dataset)):\n",
    "    print(idx)\n",
    "    image, size, path = test_dataset[idx]\n",
    "    image_bag, h_num, w_num = cut_image(image, 256)\n",
    "    mask_bag = []\n",
    "    for i in range(h_num):\n",
    "        tmp = []\n",
    "        for j in range(w_num):\n",
    "            img = image_bag[i][j]\n",
    "            x_tensor = torch.from_numpy(img).to(DEVICE).unsqueeze(0)\n",
    "            # Predict test image\n",
    "            pred_mask = best_model(x_tensor)\n",
    "            pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "            pred_mask = pred_mask[:, 128:-128, 128:-128]\n",
    "            # Convert pred_mask from `CHW` format to `HWC` format\n",
    "            pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "            # Get prediction channel corresponding to building\n",
    "            pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), class_values), target_image_dims = [size[1], size[2], size[0]])\n",
    "            tmp.append(pred_mask)\n",
    "        mask_bag.append(tmp)\n",
    "\n",
    "    mask = concat_tile(mask_bag)\n",
    "    mask = mask[:size[0], :size[1], :]\n",
    "\n",
    "    cancer = np.sum(mask == 75)\n",
    "    tumor = np.sum(mask == 0)\n",
    "    test_ratio2.append([path, cancer/(cancer+tumor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio2 = pd.DataFrame(test_ratio2, columns=[\"path\", \"ratio\"])\n",
    "test_ratio2.to_csv(\"./test_ratio2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
